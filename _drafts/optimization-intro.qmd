---
layout: post
title: "Optimization Theory - Introduction"
#date: 
description: An introductory and intuitive view of Optimization Theory
#img:  
---

- [Introduction](#introduction)
- [Hilbert Space](#hilbert)

<a name="introduction"/>
### Introduction

The field of optimization can be unified by a few geometric principles of linear vector space theory, which convey enough insight to solve complex problems. Indeed, the extension of these intuitive geometric insights to infinife-dimensional spaces is the reason why studing functional analysis becomes useful. Functional analysis should not be feared, however, as most of its main results are expressed as abstractions of intuitive geometric properties of finite-dimensional spaces. Once this intuition is build, it widens the range of problems that optimization theory can be applied to. 

A first simple result is the Projection Theorem, which states that the perpendicular from a point to a hyperplane is the shortest line connecting the two of them. This same intuition has direct extensions to infinite-dimensional Hilbert spaces and forms the basis of least-squares estimation. Extending the projection theorem to nonquadratic problems is possible due to the Hahn-Banach theorem, which stands out as preserving the essence of simple geometric ideas to more general vector spaces (even without a topology defined over it). That theorem has multiple versions, one of them being the existence of a separating hyperplane between convex sets.

To express a problem in terms of hyperplanes that was originaly stated in terms of vectors in a space is the (one of the many) principles of duality. The geometric relation is established by reframing a minimization problem over vectors as a maximization problem over hyperplanes. For example, the shortest distance between a point and a convex set is equal to the maximum of the distances from the same point to a hyperplane separating it from the convex set.

Another optimization technique is the method of differential calculus, whose extension to infinite-dimesional spces is straightforward, and the geometric intuition remains the same, i.e., the tangent hyperplane at the optimal point is horizontal.

It should be stressed that convexity, hyperplanes and duality are essential parts of optimization theory. In fact, global optimization problems in linear spaces rely on convexity to reach its results, while differential conditions suffice for local theory.

The introduction of topological properties (be it by defining a metric, a norm or an inner product), such as convergence and open and closed sets, is what distinghuishes functional analysis from linear algebra. However, optimization theory can be proven in purely algebric terms, prescinding from a topology.

Convexity and Cones are seen as generalizations of the concepts of subspaces and linear varieties, since both of these are convex cones. In fact, convexity is deemed as the fundamental algebraic concept of vector spaces. Note, however, convex cones also arise when defining positive vectors in a vector space.

The axioms of vector spaces describe algebraic properties only (such as addition, scalar multiplication and linear combinations), which makes the introduction of a measure of distance necessary to explore topological concepts like opennes, closure, convergence and completeness. In vectoral spaces this is usually done by introducing a norm, an abstraction of the concept of length. Moreover, it can be shown that interior and taking closure preserves convexity, subspaces, linear varieties and cones.

Classical examples include $l_{p}$, the sequence of real numbers $\{x_{1}, x_{2}, \dots\}$ for which $\sum_{i=1}^{\infty}{|x_{i}|^p} < \infty$ and $0 \leq p \leq \infty$. The norm is defined as $||x||_{p} = \left( \sum_{i=1}^{\infty}{|x_{i}|^p} \right)^\frac{1}{p}$ --- the space $l_{\infty}$ is the space of bounded sequences with the norm defined as $|| x ||_{\infty} = \sup_{i}{|x_{i}|}$. 

$L_{p}[a,b]$ spaces, for $1 \leq p \leq \infty $, consists of real-valued measurable functions on $[a,b]$ for which $|x(t)|^{p}$ is Lebesgue intergrable and the norm is defined as $||x||_{p} = \left( \int_{a}^{b}{|x(t)|^{p} \, dt} \right)^\frac{1}{p}$ --- when $0 < p < 1$ these are metric spaces, whose metric is not induced by a norm. Note that $||x||_{p} = 0$ on this space does not necessarily imply that $x$ is the zero vector, as it may be different from zero in a set of measure zero. However, $L_{p}[a,b]$ is a normed linear space if no distinction is made between function equal almost everywhere.

The space $L_{\infty}[a,b]$ is analogous to $l_{\infty}$, defined as the space of all bounded Lebesgue measurable functions on $[a,b]$, excepT for a set of measure zero. Since we regard as equivalent two functions differing only on a set of measure zero, defining the norm as $\sup_{a \leq t \leq b}{|x(t)|}$ is ambiguous, as this supremum is different for different functions equivalent to $x$. Therefore, the norm is defined as $||x||_{\infty} = \text{ess} \sup |x(t)| $ (essential supremum) $\equiv \inf_{y(t) = x(t) a.e.}{[\sup |y(t)|]}$.

Transformations are objects that make the study of linear spaces interesting and useful, but do not forget the need of a topology, usually induced by a norm in vectoral spaces, to define a notion of continuity (which depends on the topological structure on both spaces that the transformation is defined on).

A key concept of analysis is convergence, usually to prove the existence of a vector satisfying some desired property. To do so, it is common to define a sequence of vectors converging to a limit and, in many cases, this limit can be shown to satisfy that desired property. Spaces of particular interest are those in which every Cauchy sequence has a limit, known as complete spaces --- a complete normed space is a Banach space. Having this property allows the identification of convergence sequences without identifying their limits. Because of that, in applied problems, it is intersting to formulate it in terms of Banach space rather than in other (maybe incomplete) spaces: the reason for that is that the desired optimal vector is the limit of a sequence and the Cauchy criterion for convergence is a test for convergence, which can be applied when the limit is unknown, provided the the underlying space is complete.

The vector space of continuous functions with the norm $ ||x|| = \int_{0}^{1}{|x(t)| \, dt} $ is not complete, as there are Cauchy sequences that converge to non-continuous functions. However $C[0,1]$, the space of continuous functions with the norm $ ||x|| = \max_{0 \leq t \leq 1}{|x(t)|} $, is complete. Other examples of Banach spaces are $l_{p}$ and $L_{p}$, with $1 \leq p \leq \infty$. Moreover, completeness and closure are equivalent in Banach spaces, as a subset is closed if and only if it is complete. This is not a general statement for normed spaces though, as some normed spaces can be closed but not necessarily complete. However, in a normed linear space, any finite-dimensional subspace is complete, which is a extremely usefult result for applied work.

A fundamental question is to establish the existence of a solution to an optimization problem, to which the Weierstrass theorem, stating that a continuous funcion on a compact (closed and bounded) set has a maximum and a minimum, is of great use. It can be extended to compact sets in a normed space in infinite-dimensional problems, but compactness is too restrictive in infinite-dimensional normed spaces, making the Weierstrass theorem of limited use in these settings.

Another important concept is denseness. A set $D$ is dense in $X$ if for every $x \in X$, there are points in $D$ arbitrarily close to it. Hence, a sequence converging to a $x \in X$ can be contructed from elements of $D$. Equivalently, denseness imply that the closure of $D$ is $X$. Classic examples are the set of rationals in the real line and the space of polynomials in the space $C[a,b]$. Finally, the concept of separability, which states that a normed space is separable if it contains a countable dense set. For example, the collection of vectors $x=(x_{1}, x_{2}, \dots, x_{n}$ with rational components is countable and dense in $E^{n}$. Also, $l_{p}$ and $L_{p}$ spaces with $1 \leq p < \infty$ are separable --- $D$ is the countable set of all finitely nonzero sequences with rational components for the former, and the countable set of all polynomials with rational coefficients for the latter. Both $l_{\infty}$ and $L_{\infty}$ are not separable.

<a name="hilbert"/>
### Hilbert Space

A key concept to the projection theorem, one of the most important optimization principles, is that of orthogonality, not generally available in normed spaces. A Hilbert space is a special for of normed space, one in wchih there is an inner product defined over it and orthogonality between two vectors occur if their inner product is zero. The concepts of orthonormal bases, Fourier series, and least-squares minimization all have natural settings in Hilbert spaces.

In such settings, the norm is defined as $||x|| = \sqrt{(x|x)}$, where $(x,y)$ is the inner product between $x$ and $y$ --- a scalar satisfying symmetry [$(x|y) = (y|x)$], linearity in both entries [$(\alpha x + \beta y|z) = \alpha (x|z) + \beta (y|z)$] and non-negativity [$(x|x) \geq 0$ and $(x|x) = 0$ if and only if $x = 0$]. The Cauchy-Schwartz Inequality give more intuition that inner products define angles between vectors, which states that for all $x, y$ in an inner product space, $|(x|y) \leq ||x||||y||$, with equality holding if and only if $x = \lambda y$ or $y = 0$. These are called pre-Hilbert spaces.

Since a pre-Hilbert space is a special kind of of normed linear space, convergence, closure and completeness apply in these spaces. In fact, a Hilbert space is defined as a complete pre-Hilbert space, which is a Banach space equipped with an inner product that induces a norm. The spaces $E^{n}$ of n-tuples, $l_{2}$ and $L_{2}$ spacesare Hilbert spaces. It is also important to note that inner products are contiuous real functions.

The Classical Projection Theorem states that for any vector $v$ in a Hilbert space $V$, there is a unique vector $u_{0} \in U$, a closed subspace of $V$, such that $|| v - u_{0} || \leq || v - u ||$ for all $u \in U$. Moreover, $v - u_{0}$ being orthogonal to $U$ is a necessary and sufficient condition for $u_{0}$ to be the unique minimizing vector. There is an extended version of the theorem valid in a large class of Banach spaces, but the theorem cannot be extended to arbitrary Banach spaces.

Another important structural property of Hilbert spaces, which comes as an application of the projection theorem, is that of orthogonal complements. It states that, given any closed subset of a Hilbert space $V$, any vector can be uniquely written as the sum of a vector in $V$ and another vector in the subspace orthogonal to it. The orthogonal complement of any set $V$ is a closed subspace: the inner product guarantees that all linear combinations of elements in $V^{\perp}$ is still an element of that orthogonal space and the closure comes from the continuity of the inner product --- as the limit of any sequence of vectors orthogonal to $V$, i.e. with $0$ inner product, would have zero inner product. What motivates calling it ''orthogonal complement'' comes from the fact that the orthogonal complement of any closed subset in a Hilbert space contains additional vectors to generate the entire space --- so that, for any closed linear subspace $U$ of Hilbert space $V$, $V = U \oplus U^{\perp}$ and $U = U^{\perp \perp}$. Therefore, given a vector $v$ and closed subspace $U$ in a Hilbert space, the vector $u_{v} \in U$ such that $v - u_{v} \in U^{\perp}$ is called the orthogonal projection of $v$ onto $U$.

Orthogonal sets of vectors are convenient is various problems in Hilber spaces. One of the interesting properties is that a set of nonzero orthogonal vectors is linearly independent. Indeed, in Hilbert spaces, orthonormal sets are favored over linearly independent sets and the Gram-Schmidt orthogonalization procedure, to which the Projection Theorem is crucial, constructs orthonormal sets from a linearly independent set of vectors.

In trying to find the best approximation to $v$ in the subspace $V$ generated by orthonormal vectors $\{x_{1}, \dots, x_{n}\}$ is is a special case of the general approximation problem. Its simplicity comes from the fact that the Gram matrix of the $x_{i}$'s is the identity, so the best approximation is $\bar{v} = \sum_{i=1}^{n}{\alpha_{i}x_{i}}$, where $\alpha_{i} = (v | x_{i})$.

Consider now approximation in the closed subspace generated by an infinite orthonormal sequence, starting with a vector $v$ and forming an infinite summation with $(v | x_{i})$ as coefficients of the orthonormal vectors $x_{i}$. These coefficients are called *Fourier coefficients* of $v$ with respect to the $x_{i}$. It can be shown that for an element $v$ of a Hilbert space $V$ and an orthonormal sequence $\{x_{i}\}$ in $V$, then the series $\sum_{i=1}^{\infty}{(v | x_{i})x_{i}}$ converges to an element $\bar{v}$ in the closed subspace $U$ generated by the $x_{i}$'s. Moreover, the difference vector $v - \bar{v}$ is orthogonal to $U$. In addition, if the $x_{i}$'s span the entire space $V$, then any vector in it can be expanded as a series of the $x_{i}$'s with coefficients equal to the Fourier coefficients $(v | x_{i})$. A orthonormal sequence $\{x_{i}\}$ in a Hilber space is complete if the subspace generated by these vectors is the entire space and a necessary and sufficient condition for an orthonormal sequence to be complete is that the null vector is the only vector orthogonal to each of the $x_{i}$'s.

A small detour about the classical usage of *Fourier coefficients* in calculus. They come from the theory of Fourier series based in the complex space $L_{2}[0,2\pi]$ with inner product $(f,g) = \int_{0}^{2\pi}{f(t)\overline{g(t)}}dt$. It can be shown that the set of functions $x_{k}(t) = \frac{1}{\sqrt{2\pi}}e^{ikt}$, where $k = 0, \pm 1, \pm 2, \dots$, is an orthonormal sequence in this space. The classical complex Fourier expansion for an arbritary function $v \in L_{2}[0,2\pi]$ is $v(t) = \sum_{k=-\infty}^{\infty}{c_{k}\frac{e^{ikt}}{\sqrt{2\pi}}}$, where the Fourier coefficients $c_{k}$ come from $c_{k} = (v, x_{k}) = \frac{1}{\sqrt{2/pi}}\int_{0}^{2\pi}{v(t)e^{-ikt}dt}$.

SUppose now that you are given independent vectors $u_{1}, u_{2}, \dots, u_{n}$ which generate a subspace $U$ of a Hilbers space $V$ and wish to find a vector $\bar{v}$ in $U$ which minimizes $||v - \bar{v}||$. Rather than solving normal equations to find $\bar{v}$ as a linear combinations of the $u_{i}$'s, one can employ the Gram-Schmidt orthogonalization procedure together with Fourier series. First apply the Gram-Schmidt method to $\{u_{1}, u_{2}, \dots, u_{n}\}$ to obtain an orthonormal set $\{x_{1}, x_{2}, \dots, x_{n}\}$ generating $U$. The vector $\bar{v}$ is given by the Fourier series $\bar{v} = \sum_{i=1}^{n}{(v|x_{i})x_{i}}$ since $v - \bar{v}$ is orthogonal do $U$. Notice that the original optimization problem is easily solved once the independent vectors $u_{i}$ are orthonomalized. The advantage of this method is that once the $x_{i}$'s are found, the best approximation to any vector is easily computed. It becomes clear then that the Gran-Schmidt method can be seen as a procedure for inverting the Gran-Schmidt matrix and, conversely, algorithms for solving the normal equations have an interpretation in terms of the minimum norm problem in Hilbert space.

- Split sections