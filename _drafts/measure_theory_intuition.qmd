---
layout: post
title: "Measure Theory - Nice Intuitions"
#date: 
description: Measure theory is very abstract but intuitive thinking can help make sense of it
#img:  
---

### $\sigma$-algebras

The most natural thing is to allow for the possibility of non-measurable sets – i.e. to have our measure defined on something smaller than the power set.
$\sigma$-algebras are tricky beasts when one is dealing with infinite base sets. In fact, it is generally not possible to precisely characterize what sets are in an algebra, and which are not. For this reason, we tend to start by thinking about the events we would like to measure, and simply define the $\sigma$-algebra generated by these events.

### Constructing Probability measures

$\sigma$-algebras are tricky things to work with, while algebras are much simpler. Luckily there is an extremely powerful result that allows us to use algebras to construct our probability measures, then assume the existence of an extension to that probability measure to the $\sigma$-algebra generated by that algebra. This is Carathedory’s Extension Theorem. Thus, we can uniquely identify a probability measure on a $\sigma$-algebra by describing behavior on an algebra that generates that $\sigma$-algebra.

One can see the power of these results when defining the Lebesgue-Stieltjes Probability measure on $\mathbb{R}$. In order to do so, first define a distribution function. One question one might want to know is: what is the relationship between distribution functions and probability measures? In fact, it turns out that, using Caratedory, one can show that each distribution function induces a unique probability measure on the Borel $\sigma$-algebra of $\mathbb{R}$.

Start by using $F$ to define a probability measure on $\mathcal{A}$. But what if one wants to extend this measure to the Borel sets of $\mathbb{R}$; how one knows that such an extension exists? And if there exists more than one extension, how does one know which one to choose? Luckily, Carathedory means that one does not have to worry about these things: a unique extension exists as long as one can show that $q$ is finite on $\mathcal{A}$.

So any distribution function defines a unique probability measure on the borel sets of $\mathbb{R}$. Interestingly, the converse is true: any probability measure $p$ on the borel sets of $\mathbb{R}$ defines a probability measure as $f(x) = p((-\infty, a])$. Thus, there is a tight relationship between probability measures and distribution functions.

We can use this method (with a few technical tweaks) to define the Lebesgue measure on the real lines. This is a measure $l$ that assigns to each interval the length of that interval. This is the standard way of measuring the real line.

### Random Variables and Expectations

Consider the experiment of rolling two dice. Imagine further that what you are interested in is the sum of the numbers of the two dice. How could we talk about the probabilities of various different sums? One way would be to construct a probability space $\{Z, \Sigma, p\}$, where $X$ is the natural number between $2$ and $12$ and $p$ is the probabilities of events in this probability space generated by rolling the two dice. However, it seems that this is somewhat inefficient. After all, the underlying event here is rolling the two dice. Surely, it would be nicer to use this underlying probability space which then generates probabilities over the numbers $\{2, \dots, 12\}$. Apart from anything else, this would save us from having to generate a new probability space for every different way that we would like to combine the numbers on the two dice (for example, the product of the two numbers, or the number of die $1$ minus the number on die $2$). It is this exercise that leads us to the concept of a random variable.

It is worth noting that there is a one way relationship between measurability and continuity, in that continuous functions are measurable, but not necessarily vice versa.
One powerful useful property of measurability is that it is preserved by the act of continuously combining random variables. Thus, the sum, product, max and min of random variables are also themselves random variables.

One can now define the distribution of a random variable $x$ on $(X, \Sigma)$ - the Borel probability measure induced by it. By the distribution, we mean the probability that a random variable falls in a particular range $S$. Obviously, what we would like to do is to assign the probability of the underlying events that give rise to $S$, i.e. $p(x^{-1}(S))$. The fact that we demand that the random variable be measurable is exactly the condition that we can do this for any Borel set $S$.
There is a one to one correspondence between the following concepts:
- Borel probability measures on $\mathbb{R}$
- Distribution functions
- Random variables on $((0,1), B(0,1), l)$

The expectation of a non-negative random variable $x$ is defined as $\mathbb{E}(x) = \sup\{\mathbb{E}(z) \| z = \mathcal{L}(x)\}$, where $\mathcal{L}(x)$ is the set of all simple random variables on $X$ such that $z \leq x$. This notion is similar to that of an integral, where the weights put on any rectangle is not the length of the rectangle, but its probability. This is actually how the Lebesgue integral is defined.

One can say something about the relative expectations of random variables by knowing about their almost sure properties, such as, if $x =_{a.s.} y$, then $\mathbb{E}(x) = \mathbb{E}(y)$.

It is also true, but not easy to show, that the linearity properties of simple random variables extend to arbitrary positive random variables.

### Weak Convergence

How to put a metric structure on probability measures? Choosing a lottery is equivalent to choosing a random variable on some mother space, which is equivalent to choosing amongst probability measures. Thus, if one wants to have a model in which people choose the random variable with the highest expected utility, this concept must be well defined. To guarantee this, a continuous function on a compact set is needed. But in order to define continuity and compactness, we need metrics.

The focus will be on metricizing probability measures on the borel sets on a metric space $X$, denoted as $\Delta(X)$. If $X$ is a metric space, then at least we know how we would like to metricize two degenerate probability measures: one would simply use the distance between the two values. To go beyond this point, one need to reverse the order of events. Usually, one begins with the notion of metric, used to define a topology, and hence convergence and continuity. Here, first think of what functions one would like to be continuous, then use this to define convergence, and use this to generate a metric and then a topology.

The starting point taken is that one would like the expectation of all continuous and bounded functions to be continuous with respect to the probability measures. The route then is to define a metric on $\Delta(X)$ such that the expectations operator is continuous on continuous and bounded functions.

Let $x$ be a random variable, and note that the mapping $L_{x}: \Delta(X) \rightarrow \mathbb{R}$ defined by $L_{x}(p) = \mathbb{E}_{p}(x)$ defines a mapping from the space of all Borel probability measures to the real numbers. The discussion above suggests that one would like to define convergence in $\Delta(X)$ in such a way that $p_{n} \rightarrow p$ implies $L_{x}(p_{n}) \rightarrow L_{x}(p)$, if $x$ is a continuous bounded function.

$\{p_{m}\}$ converges weakly to $p$ in $\Delta(X)$ if $\int_{X}{\theta\,dp_{m}} \rightarrow \int_{X}{\theta\,dp}$, for every continuous, bounded function $\theta$. Equivalently, $\mathbb{E}_{p_{m}}(x) \rightarrow \mathbb{E}_{p}(x)$, for every random variable on $X$ that is continuous and bounded.

Because the definition does not start with a metric, one does not know that any sequence $p_{m}$ has a unique weak limit, but this is the case. A corollary of this is that any two borel probability measures $p, q \in \Delta(X)$ are distinct if and only if $\int_{X}{\theta\,dp} \neq \int_{X}{\theta\,dq}$, for some continuous bounded $\theta$.
  

 REFERENCES

http://www.columbia.edu/~md3405/DT_Risk_2_15.pdf