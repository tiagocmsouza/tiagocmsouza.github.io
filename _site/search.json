[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tiago Souza",
    "section": "",
    "text": "I am an economist curious about applied work in my area, with a view that Economics is a social science capable of providing an explanation to a broad range of themes.\nThinking about the mechanisms to make sense of our world is my passion, but I find it crucial to make sure you have empirial evidence to ratify that mechanics.\nThat explains my willigness to learn programming languages, but do not explain why I still love to undestand the intuition behind theoretical mathematics and statistics."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Bayesian Regression - First Assessment",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n\nreal_estate = pd.read_csv('Real estate.csv')\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      No\n      X1 transaction date\n      X2 house age\n      X3 distance to the nearest MRT station\n      X4 number of convenience stores\n      X5 latitude\n      X6 longitude\n      Y house price of unit area\n    \n  \n  \n    \n      0\n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n    \n    \n      1\n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n    \n    \n      2\n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n    \n  \n\n\n\n\n\nreal_estate.columns = ['transaction_number', 'transaction_date', 'house_age', 'distance_mrt_station', 'convenience_stores', 'latitude', 'longitude', 'price_unit_area']\nreal_estate.set_index('transaction_number', inplace = True)\nreal_estate['intercept'] = 1\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      transaction_date\n      house_age\n      distance_mrt_station\n      convenience_stores\n      latitude\n      longitude\n      price_unit_area\n      intercept\n    \n    \n      transaction_number\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n      1\n    \n    \n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n      1\n    \n    \n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n      1\n    \n  \n\n\n\n\n\nY = real_estate['price_unit_area'].to_numpy().reshape(-1,1)\nX_variables = ['intercept', 'house_age']\nX = real_estate[X_variables].to_numpy().reshape(-1, len(X_variables))\nX[1:5]\n\narray([[ 1. , 19.5],\n       [ 1. , 13.3],\n       [ 1. , 13.3],\n       [ 1. ,  5. ]])\n\n\n\nols_regression = LinearRegression(fit_intercept = False)\nols_regression.fit(X, Y)\nY_pred = ols_regression.predict(X)\n\nplt.scatter(X[:,1], Y)\nplt.plot(X[:,1], Y_pred, color='red')\nplt.xlabel('House Age')\nplt.ylabel('House Price of Unit Area')\nplt.show()\n\n\n\n\n\nalpha_ols, beta_ols = ols_regression.coef_[0]\nprint(alpha_ols, beta_ols)\n\n42.43469704626291 -0.25148841908534514"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "econometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nTiago Souza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDFM\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2022\n\n\nTiago Souza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neconometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2022\n\n\nTiago Souza\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fx_decomp/index.html",
    "href": "posts/fx_decomp/index.html",
    "title": "FX Decomposition using Dynamic Factor Models",
    "section": "",
    "text": "This post is to apply a Dynamic Factor Model to uncover common trends in currencies as well as idiosyncratic shocks that some countries might be facing.\n\\[\n\\begin{aligned}\nx^{i}_{t} = \\alpha^{i} + \\sum_{j}{\\beta^{i}_{j} f_{j} + \\varepsilon^{i}_{t}}, \\quad i = 1 \\dots n\n\\end{aligned}\n\\]\n\ndf = eurostat.get_data_df('ert_bil_eur_d', flags=False)\ndf.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'}, inplace = True)\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'BRL', 'CAD', 'CHF', 'CNY', 'CZK', 'GBP', 'HUF', 'IDR', \\\n     'JPY', 'MXN', 'NZD', 'PLN', 'TRY', 'USD', 'ZAR']\n\ndf = df[(df['statinfo'].isin(list_statinfo)) & \\\n     (df['currency'].isin(list_currency))]\ndf.drop(columns = ['unit', 'statinfo', 'freq'], inplace = True)\n\ndf = pd.melt(df, id_vars = ['currency'], var_name = 'date')\n\ndf['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\ndf.set_index(['date', 'currency'], inplace = True)\n\ndf = df.unstack()\ndf[('value', 'EUR')] = 1\ndf = df.mul(1 / df.loc[:,('value', 'USD')], axis = 0)\ndf.drop('USD', axis = 1, level = 'currency', inplace = True)\nlist_currency.remove('USD')\n\ndf = df.stack()\ndf.reset_index(level = 'currency', inplace = True)\ndf = df.pivot(columns = 'currency', values = 'value')\n#df = df.resample('M').last()\n\ndf.tail()\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      EUR\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      ZAR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-05\n      1.463541\n      5.382511\n      1.350439\n      0.928120\n      6.873220\n      22.665786\n      0.943307\n      0.832969\n      374.379775\n      15605.197623\n      132.959155\n      19.376663\n      1.587775\n      4.405245\n      18.770116\n      17.097444\n    \n    \n      2023-01-06\n      1.484762\n      5.336571\n      1.364857\n      0.939429\n      6.861429\n      22.908571\n      0.952381\n      0.842619\n      378.057143\n      15684.095238\n      134.571429\n      19.299524\n      1.608476\n      4.471429\n      18.768095\n      17.342095\n    \n    \n      2023-01-09\n      1.444091\n      5.280011\n      1.336855\n      0.922307\n      6.782536\n      22.428945\n      0.934929\n      0.823186\n      371.400524\n      15577.711294\n      132.152206\n      19.149682\n      1.565165\n      4.390707\n      18.775617\n      17.021223\n    \n    \n      2023-01-10\n      1.456309\n      5.266343\n      1.341229\n      0.923995\n      6.782803\n      22.366875\n      0.932575\n      0.823743\n      372.097361\n      15550.750723\n      132.351021\n      19.145668\n      1.574093\n      4.378439\n      18.777954\n      17.071155\n    \n    \n      2023-01-11\n      1.450451\n      5.195869\n      1.342607\n      0.927422\n      6.774635\n      22.356937\n      0.930492\n      0.825095\n      371.359449\n      15457.653299\n      132.697497\n      19.071090\n      1.573648\n      4.356472\n      18.776682\n      17.011910\n    \n  \n\n\n\n\n\nlist_em = ['BRL', 'CHF', 'CZK', 'HUF', 'IDR', 'MXN', 'PLN', 'TRY', 'ZAR']\n\nlist_dm = list(set(list_currency) - set(list_em))\n\nfactors_mq = {x: ['usd', 'em'] if x in list_em else ['usd', 'dm'] for x in list_currency}\n\n\ndf_est = df.loc['2000-01-01':].copy()\n\nendog_em = df_est[list_em].pct_change()\nendog_dm = df_est[list_dm].pct_change()\n\nplt.plot(endog_em)\nplt.legend()\n\nmodel_em = sm.tsa.DynamicFactor(endog_em, k_factors=1, factor_order=1, error_order=1)\nresult_em = model_em.fit(disp=False)\n\nmodel_dm = sm.tsa.DynamicFactor(endog_dm, k_factors=1, factor_order=1, error_order=1)\nresult_dm = model_dm.fit(disp=False)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\ndf_est['factor_em'] = result_em.factors.smoothed[0]\ndf_est['factor_dm'] = result_dm.factors.smoothed[0]\n\nendog_global = df_est[['factor_em', 'factor_dm']]\n\nplt.plot(endog_global)\nplt.legend()\n\nmodel_global = sm.tsa.DynamicFactor(endog_global, k_factors=1, factor_order=1, error_order=1)\nresult_global = model_global.fit(disp=False)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\ndf_est['factor_global'] = result_global.factors.smoothed[0]\n\nplt.plot(df_est['factor_global'])\nplt.legend()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n<matplotlib.legend.Legend at 0x13ebeb6d0>\n\n\n\n\n\n\n# model = sm.tsa.DynamicFactorMQ(\n#     endog_m, endog_quarterly=endog_q,\n#     factors=factors, factor_orders=factor_orders,\n#     factor_multiplicities=factor_multiplicities)\nprint(df_est[['factor_em', 'factor_dm', 'factor_global']].tail(15))\nfrom scipy.stats import pearsonr\ncorr, _ = pearsonr(df_est['factor_em'], df_est['factor_dm'])\nprint('Pearsons correlation: %.3f' % corr)\n\ncurrency    factor_em  factor_dm  factor_global\ndate                                           \n2022-12-21  -0.059927  -0.081435       0.735832\n2022-12-22  -0.022280   0.005360       0.186527\n2022-12-23  -0.002162   0.017472      -0.014107\n2022-12-27   0.041217  -0.045417      -0.248542\n2022-12-28   0.022716  -0.107312       0.049828\n2022-12-29  -0.069029   0.119988       0.364497\n2022-12-30  -0.017344  -0.126943       0.454206\n2023-01-02  -0.000055   0.011553      -0.005445\n2023-01-03   0.156686   0.176762      -1.906100\n2023-01-04  -0.145457  -0.232314       1.955902\n2023-01-05   0.014978   0.019381      -0.183115\n2023-01-06   0.127845   0.171858      -1.619197\n2023-01-09  -0.202377  -0.372399       2.822352\n2023-01-10  -0.006585   0.084640      -0.153977\n2023-01-11  -0.029203  -0.029067       0.331555\nPearsons correlation: 0.698"
  },
  {
    "objectID": "posts/bayesian_regression/index.html",
    "href": "posts/bayesian_regression/index.html",
    "title": "Bayesian Regression - First Assessment",
    "section": "",
    "text": "This is my first post on Bayesian Econometrics.\n\nreal_estate = pd.read_csv('Real estate.csv')\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      No\n      X1 transaction date\n      X2 house age\n      X3 distance to the nearest MRT station\n      X4 number of convenience stores\n      X5 latitude\n      X6 longitude\n      Y house price of unit area\n    \n  \n  \n    \n      0\n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n    \n    \n      1\n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n    \n    \n      2\n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n    \n  \n\n\n\n\n\nreal_estate.columns = ['transaction_number', 'transaction_date', 'house_age', 'distance_mrt_station', 'convenience_stores', 'latitude', 'longitude', 'price_unit_area']\nreal_estate.set_index('transaction_number', inplace = True)\nreal_estate['intercept'] = 1\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      transaction_date\n      house_age\n      distance_mrt_station\n      convenience_stores\n      latitude\n      longitude\n      price_unit_area\n      intercept\n    \n    \n      transaction_number\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n      1\n    \n    \n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n      1\n    \n    \n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n      1\n    \n  \n\n\n\n\n\nY = real_estate['price_unit_area'].to_numpy().reshape(-1,1)\nX_variables = ['intercept', 'house_age']\nX = real_estate[X_variables].to_numpy().reshape(-1, len(X_variables))\nX[1:5]\n\narray([[ 1. , 19.5],\n       [ 1. , 13.3],\n       [ 1. , 13.3],\n       [ 1. ,  5. ]])\n\n\n\nols_regression = LinearRegression(fit_intercept = False)\nols_regression.fit(X, Y)\nY_pred = ols_regression.predict(X)\n\nplt.scatter(X[:,1], Y)\nplt.plot(X[:,1], Y_pred, color='red')\nplt.xlabel('House Age')\nplt.ylabel('House Price of Unit Area')\nplt.show()\n\n\n\n\n\nalpha_ols, beta_ols = ols_regression.coef_[0]\nprint(alpha_ols, beta_ols)\n\n42.43469704626291 -0.25148841908534514"
  },
  {
    "objectID": "posts/fx_regression/index.html",
    "href": "posts/fx_regression/index.html",
    "title": "Currency Beta’s with respect to the USD",
    "section": "",
    "text": "This post is to check whether noise in daily data constitutes a problem to the estimation of currenies’ sensitivity with respect to the \\(USD\\), a common exercise done in financial markets.\nAssume that there exists a relationship of currency \\(x_{t}\\) with repect to the \\(USD\\) on a dily basis:\n\\[\n\\begin{aligned}\nx_{t} = \\alpha_{x} + \\beta_{x} USD_{t} + \\varepsilon_{t},\n\\end{aligned}\n\\]\nthis equation could be estimated to find the parameters of interest (a statistical relationship, with no causal meaning!).\nRemeber that the estimated coefficient is given by the simple formula:\n\\[\n\\begin{aligned}\n\\beta_{x} = \\frac{\\text{cov}(x_{t}, USD_{t})}{\\text{var}(USD_{t})}.\n\\end{aligned}\n\\]\nHowever, estimating this coefficint this way may face difficulties given the large variance of daily data. Given that, taken the parameters as fixed, nothing prevents one from resampling the data into a different frequency:\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_{t = 1}^{T}{x_{t}} & = \\frac{1}{T}\\sum_{t = 1}^{T}{\\alpha_{x}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\beta_{x} USD_{t}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\varepsilon_{t}} \\\\\n\\quad{} \\\\\n& \\Rightarrow x_{T} = \\alpha_{x} + \\beta_{x} USD_{T} + \\varepsilon_{T}\n\\end{aligned}\n\\]\nOur taks now is to test whether running the regressions over different sample frequencies yields distinct estimated coefficients.\n\n# Download and first inspection \n\ndf_daily = eurostat.get_data_df('ert_bil_eur_d', flags=False)\n\ndf_daily.tail(3)\n\n\n\n\n\n  \n    \n      \n      freq\n      statinfo\n      unit\n      currency\\TIME_PERIOD\n      1974-07-01\n      1974-07-02\n      1974-07-03\n      1974-07-04\n      1974-07-05\n      1974-07-08\n      ...\n      2023-01-09\n      2023-01-10\n      2023-01-11\n      2023-01-12\n      2023-01-13\n      2023-01-16\n      2023-01-17\n      2023-01-18\n      2023-01-19\n      2023-01-20\n    \n  \n  \n    \n      39\n      D\n      AVG\n      NAC\n      ZAR\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      18.2059\n      18.3054\n      18.2827\n      18.1417\n      18.2482\n      18.445\n      18.6027\n      18.396\n      18.6931\n      18.577\n    \n    \n      40\n      D\n      END\n      NAC\n      ALL\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      41\n      D\n      END\n      NAC\n      BAM\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3 rows × 12529 columns\n\n\n\n\n# Create a column with dates, select columns and filter values on interest\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'BRL', 'CAD', 'CHF', 'CNY', 'CZK', 'GBP', 'HUF', 'IDR', 'JPY', 'MXN', 'NZD', 'PLN', 'TRY', 'USD', 'ZAR']\n\ndf_daily.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'}, inplace = True)\n\ndf_daily = df_daily[(df_daily['statinfo'].isin(list_statinfo)) & \\\n     (df_daily['currency'].isin(list_currency))]\n\ndf_daily.drop(columns = ['unit', 'statinfo', 'freq'], inplace = True)\n\ndf_daily = pd.melt(df_daily, id_vars = ['currency'], var_name = 'date')\n\nprint(df_daily.tail(3))\nprint(df_daily.dtypes)\n\n       currency        date    value\n200397      TRY  2023-01-20  20.3566\n200398      USD  2023-01-20   1.0826\n200399      ZAR  2023-01-20  18.5770\ncurrency     object\ndate         object\nvalue       float64\ndtype: object\n\n\n\n# Transform date into the correct format and place currencies in columns\n\ndf_daily['date'] = df_daily['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\ndf_daily.set_index(['date', 'currency'], inplace = True)\n\ndf_daily = df_daily.unstack()\n\ndf_daily = df_daily.droplevel(level=0, axis=1)\n\ndf_daily.tail(3)\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      USD\n      ZAR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-18\n      1.5413\n      5.5252\n      1.4505\n      0.9906\n      7.3193\n      23.954\n      0.87530\n      396.15\n      16307.20\n      139.79\n      20.2193\n      1.6683\n      4.6983\n      20.3706\n      1.0839\n      18.3960\n    \n    \n      2023-01-19\n      1.5726\n      5.6326\n      1.4603\n      0.9921\n      7.3424\n      23.924\n      0.87648\n      396.43\n      16429.66\n      139.02\n      20.5437\n      1.6978\n      4.7063\n      20.3295\n      1.0815\n      18.6931\n    \n    \n      2023-01-20\n      1.5619\n      5.6271\n      1.4583\n      0.9962\n      7.3425\n      23.922\n      0.87600\n      395.88\n      16367.67\n      140.86\n      20.4865\n      1.6852\n      4.7100\n      20.3566\n      1.0826\n      18.5770\n    \n  \n\n\n\n\n\n# Put all currencies with respect to the USD (we downloaded currencies with respect to the EUR)\n\ndf_daily['EUR'] = 1\n\ndf_daily = df_daily.mul(1 / df_daily.loc[:,'USD'], axis = 0)\n\ndf_daily.drop('USD', axis = 1, inplace = True)\n\ndf_daily.tail(3)\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      ZAR\n      EUR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-18\n      1.421995\n      5.097518\n      1.338223\n      0.913922\n      6.752745\n      22.099825\n      0.807547\n      365.485746\n      15044.930344\n      128.969462\n      18.654212\n      1.539164\n      4.334625\n      18.793800\n      16.972045\n      0.922594\n    \n    \n      2023-01-19\n      1.454092\n      5.208137\n      1.350254\n      0.917337\n      6.789089\n      22.121128\n      0.810430\n      366.555710\n      15191.548775\n      128.543689\n      18.995562\n      1.569857\n      4.351641\n      18.797503\n      17.284420\n      0.924642\n    \n    \n      2023-01-20\n      1.442730\n      5.197765\n      1.347035\n      0.920192\n      6.782283\n      22.096804\n      0.809163\n      365.675226\n      15118.852762\n      130.112692\n      18.923425\n      1.556623\n      4.350637\n      18.803436\n      17.159616\n      0.923702\n    \n  \n\n\n\n\n\n# Dictionary with dataframes for analysis\n\nfreq_list = ['W', 'M', 'Q']\n\ndf_dict = {'D': df_daily.pct_change().dropna()}\n\ndef resample_data(df, freq):\n    \n    df_resampled = df.resample(freq) \\\n        .last() \\\n        .pct_change() \\\n        .dropna() \\\n        .to_period(freq)\n\n    return df_resampled\n\ndf_dict.update({freq: resample_data(df_daily, freq) for freq in freq_list})"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "econometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nTiago Souza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDFM\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2022\n\n\nTiago Souza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neconometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2022\n\n\nTiago Souza\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "Just an Economist",
    "section": "",
    "text": "Description 1\nContinue reading"
  },
  {
    "objectID": "posts-proj/bayesian-reg/index.html",
    "href": "posts-proj/bayesian-reg/index.html",
    "title": "Bayesian Regression - First Assessment",
    "section": "",
    "text": "This is my first post on Bayesian Econometrics.\n\nreal_estate = pd.read_csv('Real estate.csv')\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      No\n      X1 transaction date\n      X2 house age\n      X3 distance to the nearest MRT station\n      X4 number of convenience stores\n      X5 latitude\n      X6 longitude\n      Y house price of unit area\n    \n  \n  \n    \n      0\n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n    \n    \n      1\n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n    \n    \n      2\n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n    \n  \n\n\n\n\n\nreal_estate.columns = ['transaction_number', 'transaction_date', 'house_age', 'distance_mrt_station', 'convenience_stores', 'latitude', 'longitude', 'price_unit_area']\nreal_estate.set_index('transaction_number', inplace = True)\nreal_estate['intercept'] = 1\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      transaction_date\n      house_age\n      distance_mrt_station\n      convenience_stores\n      latitude\n      longitude\n      price_unit_area\n      intercept\n    \n    \n      transaction_number\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n      1\n    \n    \n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n      1\n    \n    \n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n      1\n    \n  \n\n\n\n\n\nY = real_estate['price_unit_area'].to_numpy().reshape(-1,1)\nX_variables = ['intercept', 'house_age']\nX = real_estate[X_variables].to_numpy().reshape(-1, len(X_variables))\nX[1:5]\n\narray([[ 1. , 19.5],\n       [ 1. , 13.3],\n       [ 1. , 13.3],\n       [ 1. ,  5. ]])\n\n\n\nols_regression = LinearRegression(fit_intercept = False)\nols_regression.fit(X, Y)\nY_pred = ols_regression.predict(X)\n\nplt.scatter(X[:,1], Y)\nplt.plot(X[:,1], Y_pred, color='red')\nplt.xlabel('House Age')\nplt.ylabel('House Price of Unit Area')\nplt.show()\n\n\n\n\n\nalpha_ols, beta_ols = ols_regression.coef_[0]\nprint(alpha_ols, beta_ols)\n\n42.43469704626291 -0.25148841908534514"
  },
  {
    "objectID": "posts-proj/fx-regression/index.html",
    "href": "posts-proj/fx-regression/index.html",
    "title": "Currency Beta’s with respect to the EURUSD",
    "section": "",
    "text": "This post is to check whether noise in daily data constitutes a problem in estimating currenies’ sensitivity with respect to the \\(EURUSD\\), a common exercise done in financial markets.\nAssume that there exists a relationship of currency \\(x_{t}\\) with repect to the \\(EURUSD\\) on a dily basis:\n\\[\n\\begin{aligned}\nx_{t} = \\alpha_{x} + \\beta_{x} EURUSD_{t} + \\varepsilon_{t},\n\\end{aligned}\n\\]\nthis equation could be estimated to find the parameters of interest (a statistical relationship, with no causal meaning!).\nRemeber that the estimated coefficient is given by the simple formula:\n\\[\n\\begin{aligned}\n\\beta_{x} = \\frac{\\text{cov}(x_{t}, EURUSD_{t})}{\\text{var}(EURUSD_{t})}.\n\\end{aligned}\n\\]\nHowever, estimating it this way may face difficulties given the large variance of daily data. Given that, taken the parameters as fixed, nothing prevents one from resampling the data into a different frequency:\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_{t = 1}^{T}{x_{t}} & = \\frac{1}{T}\\sum_{t = 1}^{T}{\\alpha_{x}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\beta_{x} USD_{t}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\varepsilon_{t}} \\\\\n\\quad{} \\\\\n& \\Rightarrow x_{T} = \\alpha_{x} + \\beta_{x} USD_{T} + \\varepsilon_{T},\n\\end{aligned}\n\\]\nand estimating this instead. Our taks now is to test whether running the regressions over different sample frequencies yields distinct estimated coefficients and the extent to which using daily data represents a problem.\n\n\nCode\n# Download and first inspection of data\n\ndf_daily = eurostat.get_data_df('ert_bil_eur_d', flags=False)\n\ndf_daily.tail(3)\n\n\n\n\n\n\n  \n    \n      \n      freq\n      statinfo\n      unit\n      currency\\TIME_PERIOD\n      1974-07-01\n      1974-07-02\n      1974-07-03\n      1974-07-04\n      1974-07-05\n      1974-07-08\n      ...\n      2023-01-23\n      2023-01-24\n      2023-01-25\n      2023-01-26\n      2023-01-27\n      2023-01-30\n      2023-01-31\n      2023-02-01\n      2023-02-02\n      2023-02-03\n    \n  \n  \n    \n      39\n      D\n      AVG\n      NAC\n      ZAR\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      18.6597\n      18.7687\n      18.6745\n      18.6127\n      18.7185\n      18.889\n      18.9223\n      18.8328\n      18.7046\n      18.7624\n    \n    \n      40\n      D\n      END\n      NAC\n      ALL\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      41\n      D\n      END\n      NAC\n      BAM\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3 rows × 12539 columns\n\n\n\n\n\nCode\n# Create a column with dates, select columns and filter values on interest\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'CHF', 'CZK', 'DKK', 'GBP', 'HUF', 'JPY', 'NOK', 'NZD', 'PLN', 'TRY', 'SEK', 'USD']\n\ndf_daily = df_daily.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'})\n\ndf_daily = df_daily[(df_daily['statinfo'].isin(list_statinfo)) &\n     (df_daily['currency'].isin(list_currency))]\n\ndf_daily = df_daily.drop(columns = ['unit', 'statinfo', 'freq'])\n\ndf_daily = pd.melt(df_daily, id_vars = ['currency'], var_name = 'date')\n\nprint(df_daily.tail(3))\n\nprint(df_daily.dtypes)\n\n\n       currency        date    value\n162952      SEK  2023-02-03  11.3323\n162953      TRY  2023-02-03  20.5806\n162954      USD  2023-02-03   1.0937\ncurrency     object\ndate         object\nvalue       float64\ndtype: object\n\n\n\n\nCode\n# Date into the correct format and place currencies in columns\n\ndf_daily['date'] = df_daily['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\ndf_daily = (df_daily.set_index(['date', 'currency'])\n            .unstack()\n            .droplevel(level=0, axis=1))\n\ndf_daily.tail(3)\n\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      CHF\n      CZK\n      DKK\n      GBP\n      HUF\n      JPY\n      NOK\n      NZD\n      PLN\n      SEK\n      TRY\n      USD\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-02-01\n      1.5392\n      0.9980\n      23.775\n      7.4396\n      0.88413\n      390.20\n      141.37\n      10.8569\n      1.6903\n      4.7075\n      11.3455\n      20.4978\n      1.0894\n    \n    \n      2023-02-02\n      1.5407\n      0.9992\n      23.809\n      7.4393\n      0.89289\n      387.20\n      141.12\n      10.9535\n      1.6855\n      4.7015\n      11.3587\n      20.6766\n      1.0988\n    \n    \n      2023-02-03\n      1.5499\n      0.9989\n      23.725\n      7.4443\n      0.89250\n      386.58\n      140.45\n      10.9783\n      1.6886\n      4.6920\n      11.3323\n      20.5806\n      1.0937\n    \n  \n\n\n\n\n\n\nCode\n# Pll currencies with respect to the USD (we downloaded currencies with respect to the EUR)\n\ndf_daily['EUR'] = 1\n\ndf_daily = (df_daily.mul(1 / df_daily.loc[:,'USD'], axis = 0)\n            .drop('USD', axis = 1))\n\ndf_daily.tail(3)\n\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      CHF\n      CZK\n      DKK\n      GBP\n      HUF\n      JPY\n      NOK\n      NZD\n      PLN\n      SEK\n      TRY\n      EUR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-02-01\n      1.412888\n      0.916101\n      21.823940\n      6.829080\n      0.811575\n      358.178814\n      129.768680\n      9.965945\n      1.551588\n      4.321186\n      10.414448\n      18.815678\n      0.917936\n    \n    \n      2023-02-02\n      1.402166\n      0.909356\n      21.668183\n      6.770386\n      0.812605\n      352.384419\n      128.431016\n      9.968602\n      1.533946\n      4.278759\n      10.337368\n      18.817437\n      0.910084\n    \n    \n      2023-02-03\n      1.417116\n      0.913322\n      21.692420\n      6.806528\n      0.816037\n      353.460730\n      128.417299\n      10.037762\n      1.543933\n      4.290025\n      10.361434\n      18.817409\n      0.914328\n    \n  \n\n\n\n\n\n\nCode\n# Dictionary with dataframes for analysis\n\nfreq_list = ['W', 'M', 'Q']\n\ndf_dict = {'D': df_daily.pct_change().dropna()}\n\ndef resample_data(df, freq):\n    \n    df_resampled = (df.resample(freq)\n                    .last()\n                    .pct_change()\n                    .dropna()\n                    .to_period(freq))\n\n    return df_resampled\n\ndf_dict.update({freq: resample_data(df_daily, freq) for freq in freq_list})\n\n\n\n\nCode\n# Run regressions for every currency and frequency\n# Note that statsmodels handles multiple Y's separately\n\ncoeffs = []\n\nfor freq, data in df_dict.items():\n\n    data_x = sm.add_constant(data['EUR'])\n    data_y = data.drop(columns=['EUR'])\n    models = sm.OLS(data_y, data_x).fit()\n    params = (models.params.T[['EUR']].set_index(data_y.columns)\n                .rename(columns={'EUR': freq}))\n    coeffs.append(params)\n\ncoeffs = pd.concat(coeffs, axis='columns')\ncoeffs.tail(3)\n\n\n\n\n\n\n  \n    \n      \n      D\n      W\n      M\n      Q\n    \n    \n      currency\n      \n      \n      \n      \n    \n  \n  \n    \n      PLN\n      0.856764\n      1.005148\n      1.012613\n      0.969457\n    \n    \n      SEK\n      0.953337\n      0.964236\n      0.993492\n      0.949884\n    \n    \n      TRY\n      0.477494\n      0.593587\n      0.525770\n      0.560618\n    \n  \n\n\n\n\n\n\nCode\ncoeffs_plot = (coeffs.div(coeffs['D'], axis='rows')\n                .drop(columns=['D']))\n            \ncoeffs_plot.plot(kind='density')\ncoeffs_plot.plot(kind='box')\n\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts-proj/fx-decomp/index.html",
    "href": "posts-proj/fx-decomp/index.html",
    "title": "FX Decomposition using Dynamic Factor Models",
    "section": "",
    "text": "This post is to apply a Dynamic Factor Model to uncover common trends in currencies as well as idiosyncratic shocks that some countries might be facing.\n\\[\n\\begin{aligned}\nx^{i}_{t} = \\alpha^{i} + \\sum_{j}{\\beta^{i}_{j} f_{j} + \\varepsilon^{i}_{t}}, \\quad i = 1 \\dots n\n\\end{aligned}\n\\]\n\ndf = eurostat.get_data_df('ert_bil_eur_d', flags=False)\ndf.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'}, inplace = True)\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'BRL', 'CAD', 'CHF', 'CNY', 'CZK', 'GBP', 'HUF', 'IDR', \\\n     'JPY', 'MXN', 'NZD', 'PLN', 'TRY', 'USD', 'ZAR']\n\ndf = df[(df['statinfo'].isin(list_statinfo)) & \\\n     (df['currency'].isin(list_currency))]\ndf.drop(columns = ['unit', 'statinfo', 'freq'], inplace = True)\n\ndf = pd.melt(df, id_vars = ['currency'], var_name = 'date')\n\ndf['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\ndf.set_index(['date', 'currency'], inplace = True)\n\ndf = df.unstack()\ndf[('value', 'EUR')] = 1\ndf = df.mul(1 / df.loc[:,('value', 'USD')], axis = 0)\ndf.drop('USD', axis = 1, level = 'currency', inplace = True)\nlist_currency = [x if x != 'USD' else 'EUR' for x in list_currency]\n\ndf = df.stack()\ndf.reset_index(level = 'currency', inplace = True)\ndf = df.pivot(columns = 'currency', values = 'value')\ndf = df.resample('M') \\\n     .last() \\\n     .pct_change() \\\n     .to_period('M')\n\ndf.tail()\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      EUR\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      ZAR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-10\n      0.012801\n      -0.014687\n      -0.005591\n      0.020690\n      0.025950\n      -0.019187\n      -0.016744\n      -0.041075\n      -0.045926\n      0.024687\n      0.027813\n      -0.013094\n      -0.021209\n      -0.045096\n      0.003488\n      0.019047\n    \n    \n      2022-11\n      -0.050925\n      -0.001570\n      -0.011532\n      -0.051361\n      -0.030573\n      -0.050379\n      -0.044526\n      -0.040387\n      -0.047441\n      0.003728\n      -0.064750\n      -0.030038\n      -0.070510\n      -0.053657\n      0.000881\n      -0.075902\n    \n    \n      2022-12\n      -0.010287\n      -0.003815\n      0.001882\n      -0.027880\n      -0.025268\n      -0.036063\n      -0.027189\n      -0.002388\n      -0.045126\n      -0.012362\n      -0.051597\n      0.013884\n      -0.017598\n      -0.023580\n      0.004592\n      0.001690\n    \n    \n      2023-01\n      -0.029031\n      -0.033104\n      -0.006552\n      0.003082\n      -0.020554\n      -0.028644\n      -0.015416\n      -0.022299\n      -0.039879\n      -0.029556\n      -0.011146\n      -0.037127\n      -0.011899\n      -0.009484\n      0.004991\n      0.029394\n    \n    \n      2023-02\n      -0.008037\n      -0.008776\n      -0.006382\n      -0.013755\n      -0.002865\n      -0.012298\n      -0.009509\n      0.003728\n      -0.020480\n      -0.007676\n      -0.015258\n      -0.006284\n      -0.007864\n      -0.013085\n      0.000304\n      -0.017879\n    \n  \n\n\n\n\n\nlist_em = ['BRL', 'CHF', 'CZK', 'HUF', 'IDR', 'MXN', 'PLN', 'TRY', 'ZAR']\n\nlist_dm = list(set(list_currency) - set(list_em))\n\nfactors_mq = {x: ['usd', 'em'] if x in list_em else ['usd', 'dm'] for x in list_currency}\n\n\ndf_est = df.loc['2000-01':].copy()\n\ndf_est.plot()\nplt.show()\n\nmodel = sm.tsa.DynamicFactorMQ(df_est, factors=factors_mq)\nresult = model.fit(disp=False)\n\n\n\n\n\ndf_est['factor_usd'] = result.factors.smoothed['usd']\ndf_est['factor_em'] = result.factors.smoothed['em']\ndf_est['factor_dm'] = result.factors.smoothed['dm']\n\ndf_est['factor_usd'].plot()\nplt.show()"
  },
  {
    "objectID": "posts-theory/economics-personal-view.html",
    "href": "posts-theory/economics-personal-view.html",
    "title": "Economics - A Personal Statement",
    "section": "",
    "text": "The most important lesson I learned a few years ago is that Economics studies people, how they behave and how we economists, as social scientists, try to understand the main drivers of people’s choices. In some sense, it becomes a fun quest to find what motivates shoppers to buy a product, citizens to abide by the rules of where they reside or burglars to commit wrongdoings, children to obey their parents or parents thinking about a second child, policymakers to propose laws that will benefit the general population or their chances for reelection… The list goes on and on. Whenever people are making choices, Economics can provide one point of view. As Gabry Becker put very well in his “Economic Aproach to Human Behavior”, Economics is a set of tools that can shed light on social phenomena from a perspective different than other social sciences as Psychology, Anthropology, History, Sociology.\nEconomics is about weighing benefits and costs, but the cost is not necessarily the price announced or the one you see on the shelves. Often times costs are subjective, called shadow price. One does not know exactly how much she would pay for a bus on a rainy day, but it is certainly more that the ticket accquired. Economists can have a grasp of this value based on behavior and people’s choices and that is one of the reasons I find empirical work crucial for Economics. It creates the bridge between what we expect from models and the real world, serving as a reality check.\nWe often rely on models to reach conclusions, but there are two dimensions that economists should pay special attention to before making bold statments. The first layer is to ask if that model is suited to understand the problem at hand by making sure that it contains the correct drivers of behavior. Without that, one might be looking to the weather to determine what people will watch on television (which might even have an influence, but should not be the determining factor). However, even with the understanding of the main drivers of behavior, an economist should always give a second step and question whether there are other aspects of life influencing people’s choices in that particular setting. Putting differently, being aware of the limitations that a model has in providing guidance to understand behavior and to serve as a guidance for policy."
  },
  {
    "objectID": "theory.html#economics---my-personal-view-1",
    "href": "theory.html#economics---my-personal-view-1",
    "title": "Just an Economist",
    "section": "Economics - My Personal View ",
    "text": "Economics - My Personal View \n\n\n\n\n\n\nDescription 2\nContinue reading"
  },
  {
    "objectID": "posts/economics-personal-view.html",
    "href": "posts/economics-personal-view.html",
    "title": "Economics - A Personal Statement",
    "section": "",
    "text": "The most important lesson I learned a few years ago is that Economics studies people, how they behave and how we economists, as social scientists, try to understand the main drivers of people’s choices. In some sense, it becomes a fun quest to find what motivates shoppers to buy a product, citizens to abide by the rules of where they reside or burglars to commit wrongdoings, children to obey their parents or parents thinking about a second child, policymakers to propose laws that will benefit the general population or their chances for reelection… The list goes on and on. Whenever people are making choices, Economics can provide one point of view. As Gabry Becker put very well in his “Economic Aproach to Human Behavior”, Economics is a set of tools that can shed light on social phenomena from a perspective different than other social sciences as Psychology, Anthropology, History, Sociology.\nEconomics is about weighing benefits and costs, but the cost is not necessarily the price announced or the one you see on the shelves. Often times costs are subjective, called shadow price. One does not know exactly how much she would pay for a bus on a rainy day, but it is certainly more that the ticket accquired. Economists can have a grasp of this value based on behavior and people’s choices and that is one of the reasons I find empirical work crucial for Economics. It creates the bridge between what we expect from models and the real world, serving as a reality check.\nWe often rely on models to reach conclusions, but there are two dimensions that economists should pay special attention to before making bold statments. The first layer is to ask if that model is suited to understand the problem at hand by making sure that it contains the correct drivers of behavior. Without that, one might be looking to the weather to determine what people will watch on television (which might even have an influence, but should not be the determining factor). However, even with the understanding of the main drivers of behavior, an economist should always give a second step and question whether there are other aspects of life influencing people’s choices in that particular setting. Putting differently, being aware of the limitations that a model has in providing guidance to understand behavior and to serve as a guidance for policy."
  },
  {
    "objectID": "projects/bayesian-reg/index.html",
    "href": "projects/bayesian-reg/index.html",
    "title": "Bayesian Regression - First Assessment",
    "section": "",
    "text": "This is my first post on Bayesian Econometrics.\n\n\nCode\nreal_estate = pd.read_csv('Real estate.csv')\nreal_estate.head(3)\n\n\n\n\n\n\n  \n    \n      \n      No\n      X1 transaction date\n      X2 house age\n      X3 distance to the nearest MRT station\n      X4 number of convenience stores\n      X5 latitude\n      X6 longitude\n      Y house price of unit area\n    \n  \n  \n    \n      0\n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n    \n    \n      1\n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n    \n    \n      2\n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n    \n  \n\n\n\n\n\n\nCode\nreal_estate.columns = ['transaction_number', 'transaction_date', 'house_age', 'distance_mrt_station', 'convenience_stores', 'latitude', 'longitude', 'price_unit_area']\nreal_estate.set_index('transaction_number', inplace = True)\nreal_estate['intercept'] = 1\nreal_estate.head(3)\n\n\n\n\n\n\n  \n    \n      \n      transaction_date\n      house_age\n      distance_mrt_station\n      convenience_stores\n      latitude\n      longitude\n      price_unit_area\n      intercept\n    \n    \n      transaction_number\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n      1\n    \n    \n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n      1\n    \n    \n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n      1\n    \n  \n\n\n\n\n\n\nCode\nY = real_estate['price_unit_area'].to_numpy().reshape(-1,1)\nX_variables = ['intercept', 'house_age']\nX = real_estate[X_variables].to_numpy().reshape(-1, len(X_variables))\nX[1:5]\n\n\narray([[ 1. , 19.5],\n       [ 1. , 13.3],\n       [ 1. , 13.3],\n       [ 1. ,  5. ]])\n\n\n\n\nCode\nols_regression = LinearRegression(fit_intercept = False)\nols_regression.fit(X, Y)\nY_pred = ols_regression.predict(X)\n\nplt.scatter(X[:,1], Y)\nplt.plot(X[:,1], Y_pred, color='red')\nplt.xlabel('House Age')\nplt.ylabel('House Price of Unit Area')\nplt.show()\n\n\n\n\n\n\n\nCode\nalpha_ols, beta_ols = ols_regression.coef_[0]\nprint(alpha_ols, beta_ols)\n\n\n42.43469704626291 -0.25148841908534514"
  },
  {
    "objectID": "projects/fx-regression/index.html",
    "href": "projects/fx-regression/index.html",
    "title": "Currency Beta’s with respect to the EURUSD",
    "section": "",
    "text": "This post is to check whether noise in daily data constitutes a problem in estimating currenies’ sensitivity with respect to the \\(EURUSD\\), a common exercise done in financial markets.\nAssume that there exists a relationship of currency \\(x_{t}\\) with repect to the \\(EURUSD\\) on a dily basis:\n\\[\n\\begin{aligned}\nx_{t} = \\alpha_{x} + \\beta_{x} EURUSD_{t} + \\varepsilon_{t},\n\\end{aligned}\n\\]\nthis equation could be estimated to find the parameters of interest (a statistical relationship, with no causal meaning!).\nRemeber that the estimated coefficient is given by the simple formula:\n\\[\n\\begin{aligned}\n\\beta_{x} = \\frac{\\text{cov}(x_{t}, EURUSD_{t})}{\\text{var}(EURUSD_{t})}.\n\\end{aligned}\n\\]\nHowever, estimating it this way may face difficulties given the large variance of daily data. Given that, taken the parameters as fixed, nothing prevents one from resampling the data into a different frequency:\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_{t = 1}^{T}{x_{t}} & = \\frac{1}{T}\\sum_{t = 1}^{T}{\\alpha_{x}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\beta_{x} USD_{t}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\varepsilon_{t}} \\\\\n\\quad{} \\\\\n& \\Rightarrow x_{T} = \\alpha_{x} + \\beta_{x} USD_{T} + \\varepsilon_{T},\n\\end{aligned}\n\\]\nand estimating this instead. Our taks now is to test whether running the regressions over different sample frequencies yields distinct estimated coefficients and the extent to which using daily data represents a problem.\n\n\nCode\n# Download and first inspection of data\n\ndf_daily = eurostat.get_data_df('ert_bil_eur_d', flags=False)\n\ndf_daily.tail(3)\n\n\n\n\n\n\n  \n    \n      \n      freq\n      statinfo\n      unit\n      currency\\TIME_PERIOD\n      1974-07-01\n      1974-07-02\n      1974-07-03\n      1974-07-04\n      1974-07-05\n      1974-07-08\n      ...\n      2023-01-23\n      2023-01-24\n      2023-01-25\n      2023-01-26\n      2023-01-27\n      2023-01-30\n      2023-01-31\n      2023-02-01\n      2023-02-02\n      2023-02-03\n    \n  \n  \n    \n      39\n      D\n      AVG\n      NAC\n      ZAR\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      18.6597\n      18.7687\n      18.6745\n      18.6127\n      18.7185\n      18.889\n      18.9223\n      18.8328\n      18.7046\n      18.7624\n    \n    \n      40\n      D\n      END\n      NAC\n      ALL\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      41\n      D\n      END\n      NAC\n      BAM\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3 rows × 12539 columns\n\n\n\n\n\nCode\n# Create a column with dates, select columns and filter values on interest\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'CHF', 'CZK', 'DKK', 'GBP', 'HUF', 'JPY', 'NOK', 'NZD', 'PLN', 'TRY', 'SEK', 'USD']\n\ndf_daily = df_daily.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'})\n\ndf_daily = df_daily[(df_daily['statinfo'].isin(list_statinfo)) &\n     (df_daily['currency'].isin(list_currency))]\n\ndf_daily = df_daily.drop(columns = ['unit', 'statinfo', 'freq'])\n\ndf_daily = pd.melt(df_daily, id_vars = ['currency'], var_name = 'date')\n\nprint(df_daily.tail(3))\n\nprint(df_daily.dtypes)\n\n\n       currency        date    value\n162952      SEK  2023-02-03  11.3323\n162953      TRY  2023-02-03  20.5806\n162954      USD  2023-02-03   1.0937\ncurrency     object\ndate         object\nvalue       float64\ndtype: object\n\n\n\n\nCode\n# Date into the correct format and place currencies in columns\n\ndf_daily['date'] = df_daily['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\ndf_daily = (df_daily.set_index(['date', 'currency'])\n            .unstack()\n            .droplevel(level=0, axis=1))\n\ndf_daily.tail(3)\n\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      CHF\n      CZK\n      DKK\n      GBP\n      HUF\n      JPY\n      NOK\n      NZD\n      PLN\n      SEK\n      TRY\n      USD\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-02-01\n      1.5392\n      0.9980\n      23.775\n      7.4396\n      0.88413\n      390.20\n      141.37\n      10.8569\n      1.6903\n      4.7075\n      11.3455\n      20.4978\n      1.0894\n    \n    \n      2023-02-02\n      1.5407\n      0.9992\n      23.809\n      7.4393\n      0.89289\n      387.20\n      141.12\n      10.9535\n      1.6855\n      4.7015\n      11.3587\n      20.6766\n      1.0988\n    \n    \n      2023-02-03\n      1.5499\n      0.9989\n      23.725\n      7.4443\n      0.89250\n      386.58\n      140.45\n      10.9783\n      1.6886\n      4.6920\n      11.3323\n      20.5806\n      1.0937\n    \n  \n\n\n\n\n\n\nCode\n# Pll currencies with respect to the USD (we downloaded currencies with respect to the EUR)\n\ndf_daily['EUR'] = 1\n\ndf_daily = (df_daily.mul(1 / df_daily.loc[:,'USD'], axis = 0)\n            .drop('USD', axis = 1))\n\ndf_daily.tail(3)\n\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      CHF\n      CZK\n      DKK\n      GBP\n      HUF\n      JPY\n      NOK\n      NZD\n      PLN\n      SEK\n      TRY\n      EUR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-02-01\n      1.412888\n      0.916101\n      21.823940\n      6.829080\n      0.811575\n      358.178814\n      129.768680\n      9.965945\n      1.551588\n      4.321186\n      10.414448\n      18.815678\n      0.917936\n    \n    \n      2023-02-02\n      1.402166\n      0.909356\n      21.668183\n      6.770386\n      0.812605\n      352.384419\n      128.431016\n      9.968602\n      1.533946\n      4.278759\n      10.337368\n      18.817437\n      0.910084\n    \n    \n      2023-02-03\n      1.417116\n      0.913322\n      21.692420\n      6.806528\n      0.816037\n      353.460730\n      128.417299\n      10.037762\n      1.543933\n      4.290025\n      10.361434\n      18.817409\n      0.914328\n    \n  \n\n\n\n\n\n\nCode\n# Dictionary with dataframes for analysis\n\nfreq_list = ['W', 'M', 'Q']\n\ndf_dict = {'D': df_daily.pct_change().dropna()}\n\ndef resample_data(df, freq):\n    \n    df_resampled = (df.resample(freq)\n                    .last()\n                    .pct_change()\n                    .dropna()\n                    .to_period(freq))\n\n    return df_resampled\n\ndf_dict.update({freq: resample_data(df_daily, freq) for freq in freq_list})\n\n\n\n\nCode\n# Run regressions for every currency and frequency\n# Note that statsmodels handles multiple Y's separately\n\ncoeffs = []\n\nfor freq, data in df_dict.items():\n\n    data_x = sm.add_constant(data['EUR'])\n    data_y = data.drop(columns=['EUR'])\n    models = sm.OLS(data_y, data_x).fit()\n    params = (models.params.T[['EUR']].set_index(data_y.columns)\n                .rename(columns={'EUR': freq}))\n    coeffs.append(params)\n\ncoeffs = pd.concat(coeffs, axis='columns')\ncoeffs.tail(3)\n\n\n\n\n\n\n  \n    \n      \n      D\n      W\n      M\n      Q\n    \n    \n      currency\n      \n      \n      \n      \n    \n  \n  \n    \n      PLN\n      0.856764\n      1.005148\n      1.012613\n      0.969457\n    \n    \n      SEK\n      0.953337\n      0.964236\n      0.993492\n      0.949884\n    \n    \n      TRY\n      0.477494\n      0.593587\n      0.525770\n      0.560618\n    \n  \n\n\n\n\n\n\nCode\ncoeffs_plot = (coeffs.div(coeffs['D'], axis='rows')\n                .drop(columns=['D']))\n            \ncoeffs_plot.plot(kind='density')\ncoeffs_plot.plot(kind='box')\n\n\n<AxesSubplot:>"
  },
  {
    "objectID": "projects/fx-decomp/index.html",
    "href": "projects/fx-decomp/index.html",
    "title": "FX Decomposition using Dynamic Factor Models",
    "section": "",
    "text": "This post is to apply a Dynamic Factor Model to uncover common trends in currencies as well as idiosyncratic shocks that some countries might be facing.\n\\[\n\\begin{aligned}\nx^{i}_{t} = \\alpha^{i} + \\sum_{j}{\\beta^{i}_{j} f_{j} + \\varepsilon^{i}_{t}}, \\quad i = 1 \\dots n\n\\end{aligned}\n\\]\n\n\nCode\ndf = eurostat.get_data_df('ert_bil_eur_d', flags=False)\ndf.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'}, inplace = True)\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'BRL', 'CAD', 'CHF', 'CNY', 'CZK', 'GBP', 'HUF', 'IDR', \\\n     'JPY', 'MXN', 'NZD', 'PLN', 'TRY', 'USD', 'ZAR']\n\ndf = df[(df['statinfo'].isin(list_statinfo)) & \\\n     (df['currency'].isin(list_currency))]\ndf.drop(columns = ['unit', 'statinfo', 'freq'], inplace = True)\n\ndf = pd.melt(df, id_vars = ['currency'], var_name = 'date')\n\ndf['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\ndf.set_index(['date', 'currency'], inplace = True)\n\ndf = df.unstack()\ndf[('value', 'EUR')] = 1\ndf = df.mul(1 / df.loc[:,('value', 'USD')], axis = 0)\ndf.drop('USD', axis = 1, level = 'currency', inplace = True)\nlist_currency = [x if x != 'USD' else 'EUR' for x in list_currency]\n\ndf = df.stack()\ndf.reset_index(level = 'currency', inplace = True)\ndf = df.pivot(columns = 'currency', values = 'value')\ndf = df.resample('M') \\\n     .last() \\\n     .pct_change() \\\n     .to_period('M')\n\ndf.tail()\n\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      EUR\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      ZAR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-10\n      0.012801\n      -0.014687\n      -0.005591\n      0.020690\n      0.025950\n      -0.019187\n      -0.016744\n      -0.041075\n      -0.045926\n      0.024687\n      0.027813\n      -0.013094\n      -0.021209\n      -0.045096\n      0.003488\n      0.019047\n    \n    \n      2022-11\n      -0.050925\n      -0.001570\n      -0.011532\n      -0.051361\n      -0.030573\n      -0.050379\n      -0.044526\n      -0.040387\n      -0.047441\n      0.003728\n      -0.064750\n      -0.030038\n      -0.070510\n      -0.053657\n      0.000881\n      -0.075902\n    \n    \n      2022-12\n      -0.010287\n      -0.003815\n      0.001882\n      -0.027880\n      -0.025268\n      -0.036063\n      -0.027189\n      -0.002388\n      -0.045126\n      -0.012362\n      -0.051597\n      0.013884\n      -0.017598\n      -0.023580\n      0.004592\n      0.001690\n    \n    \n      2023-01\n      -0.029031\n      -0.033104\n      -0.006552\n      0.003082\n      -0.020554\n      -0.028644\n      -0.015416\n      -0.022299\n      -0.039879\n      -0.029556\n      -0.011146\n      -0.037127\n      -0.011899\n      -0.009484\n      0.004991\n      0.029394\n    \n    \n      2023-02\n      -0.008037\n      -0.008776\n      -0.006382\n      -0.013755\n      -0.002865\n      -0.012298\n      -0.009509\n      0.003728\n      -0.020480\n      -0.007676\n      -0.015258\n      -0.006284\n      -0.007864\n      -0.013085\n      0.000304\n      -0.017879\n    \n  \n\n\n\n\n\n\nCode\nlist_em = ['BRL', 'CHF', 'CZK', 'HUF', 'IDR', 'MXN', 'PLN', 'TRY', 'ZAR']\n\nlist_dm = list(set(list_currency) - set(list_em))\n\nfactors_mq = {x: ['usd', 'em'] if x in list_em else ['usd', 'dm'] for x in list_currency}\n\n\n\n\nCode\ndf_est = df.loc['2000-01':].copy()\n\ndf_est.plot()\nplt.show()\n\nmodel = sm.tsa.DynamicFactorMQ(df_est, factors=factors_mq)\nresult = model.fit(disp=False)\n\n\n\n\n\n\n\nCode\ndf_est['factor_usd'] = result.factors.smoothed['usd']\ndf_est['factor_em'] = result.factors.smoothed['em']\ndf_est['factor_dm'] = result.factors.smoothed['dm']\n\ndf_est['factor_usd'].plot()\nplt.show()"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Just an Economist",
    "section": "",
    "text": "Description 1\nContinue reading"
  },
  {
    "objectID": "posts.html#economics---my-personal-view-1",
    "href": "posts.html#economics---my-personal-view-1",
    "title": "Just an Economist",
    "section": "Economics - My Personal View ",
    "text": "Economics - My Personal View \n\n\n\n\n\n\nDescription 2\nContinue reading"
  }
]