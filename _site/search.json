[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentation",
    "section": "",
    "text": "I am an economist curious about applied work in my area, with a view that Economics is a social science capable of providing an explanation to a broad range of themes.\nThinking about the mechanisms to make sense of our world is my passion, but I find it crucial to make sure you have empirial evidence to ratify that mechanics.\nThat explains my willigness to learn programming languages, but do not explain why I still love to undestand the intuition behind theoretical mathematics and statistics."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Bayesian Regression - First Assessment",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n\nreal_estate = pd.read_csv('Real estate.csv')\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      No\n      X1 transaction date\n      X2 house age\n      X3 distance to the nearest MRT station\n      X4 number of convenience stores\n      X5 latitude\n      X6 longitude\n      Y house price of unit area\n    \n  \n  \n    \n      0\n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n    \n    \n      1\n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n    \n    \n      2\n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n    \n  \n\n\n\n\n\nreal_estate.columns = ['transaction_number', 'transaction_date', 'house_age', 'distance_mrt_station', 'convenience_stores', 'latitude', 'longitude', 'price_unit_area']\nreal_estate.set_index('transaction_number', inplace = True)\nreal_estate['intercept'] = 1\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      transaction_date\n      house_age\n      distance_mrt_station\n      convenience_stores\n      latitude\n      longitude\n      price_unit_area\n      intercept\n    \n    \n      transaction_number\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n      1\n    \n    \n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n      1\n    \n    \n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n      1\n    \n  \n\n\n\n\n\nY = real_estate['price_unit_area'].to_numpy().reshape(-1,1)\nX_variables = ['intercept', 'house_age']\nX = real_estate[X_variables].to_numpy().reshape(-1, len(X_variables))\nX[1:5]\n\narray([[ 1. , 19.5],\n       [ 1. , 13.3],\n       [ 1. , 13.3],\n       [ 1. ,  5. ]])\n\n\n\nols_regression = LinearRegression(fit_intercept = False)\nols_regression.fit(X, Y)\nY_pred = ols_regression.predict(X)\n\nplt.scatter(X[:,1], Y)\nplt.plot(X[:,1], Y_pred, color='red')\nplt.xlabel('House Age')\nplt.ylabel('House Price of Unit Area')\nplt.show()\n\n\n\n\n\nalpha_ols, beta_ols = ols_regression.coef_[0]\nprint(alpha_ols, beta_ols)\n\n42.43469704626291 -0.25148841908534514"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "econometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nTiago Souza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDFM\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2022\n\n\nTiago Souza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neconometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2022\n\n\nTiago Souza\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fx_decomp/index.html",
    "href": "posts/fx_decomp/index.html",
    "title": "FX Decomposition using Dynamic Factor Models",
    "section": "",
    "text": "This post is to apply a Dynamic Factor Model to uncover common trends in currencies as well as idiosyncratic shocks that some countries might be facing.\n\\[\n\\begin{aligned}\nx^{i}_{t} = \\alpha^{i} + \\sum_{j}{\\beta^{i}_{j} f_{j} + \\varepsilon^{i}_{t}}, \\quad i = 1 \\dots n\n\\end{aligned}\n\\]\n\ndf = eurostat.get_data_df('ert_bil_eur_d', flags=False)\ndf.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'}, inplace = True)\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'BRL', 'CAD', 'CHF', 'CNY', 'CZK', 'GBP', 'HUF', 'IDR', \\\n     'JPY', 'MXN', 'NZD', 'PLN', 'TRY', 'USD', 'ZAR']\n\ndf = df[(df['statinfo'].isin(list_statinfo)) & \\\n     (df['currency'].isin(list_currency))]\ndf.drop(columns = ['unit', 'statinfo', 'freq'], inplace = True)\n\ndf = pd.melt(df, id_vars = ['currency'], var_name = 'date')\n\ndf['date'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\ndf.set_index(['date', 'currency'], inplace = True)\n\ndf = df.unstack()\ndf[('value', 'EUR')] = 1\ndf = df.mul(1 / df.loc[:,('value', 'USD')], axis = 0)\ndf.drop('USD', axis = 1, level = 'currency', inplace = True)\nlist_currency.remove('USD')\n\ndf = df.stack()\ndf.reset_index(level = 'currency', inplace = True)\ndf = df.pivot(columns = 'currency', values = 'value')\n#df = df.resample('M').last()\n\ndf.tail()\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      EUR\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      ZAR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-05\n      1.463541\n      5.382511\n      1.350439\n      0.928120\n      6.873220\n      22.665786\n      0.943307\n      0.832969\n      374.379775\n      15605.197623\n      132.959155\n      19.376663\n      1.587775\n      4.405245\n      18.770116\n      17.097444\n    \n    \n      2023-01-06\n      1.484762\n      5.336571\n      1.364857\n      0.939429\n      6.861429\n      22.908571\n      0.952381\n      0.842619\n      378.057143\n      15684.095238\n      134.571429\n      19.299524\n      1.608476\n      4.471429\n      18.768095\n      17.342095\n    \n    \n      2023-01-09\n      1.444091\n      5.280011\n      1.336855\n      0.922307\n      6.782536\n      22.428945\n      0.934929\n      0.823186\n      371.400524\n      15577.711294\n      132.152206\n      19.149682\n      1.565165\n      4.390707\n      18.775617\n      17.021223\n    \n    \n      2023-01-10\n      1.456309\n      5.266343\n      1.341229\n      0.923995\n      6.782803\n      22.366875\n      0.932575\n      0.823743\n      372.097361\n      15550.750723\n      132.351021\n      19.145668\n      1.574093\n      4.378439\n      18.777954\n      17.071155\n    \n    \n      2023-01-11\n      1.450451\n      5.195869\n      1.342607\n      0.927422\n      6.774635\n      22.356937\n      0.930492\n      0.825095\n      371.359449\n      15457.653299\n      132.697497\n      19.071090\n      1.573648\n      4.356472\n      18.776682\n      17.011910\n    \n  \n\n\n\n\n\nlist_em = ['BRL', 'CHF', 'CZK', 'HUF', 'IDR', 'MXN', 'PLN', 'TRY', 'ZAR']\n\nlist_dm = list(set(list_currency) - set(list_em))\n\nfactors_mq = {x: ['usd', 'em'] if x in list_em else ['usd', 'dm'] for x in list_currency}\n\n\ndf_est = df.loc['2000-01-01':].copy()\n\nendog_em = df_est[list_em].pct_change()\nendog_dm = df_est[list_dm].pct_change()\n\nplt.plot(endog_em)\nplt.legend()\n\nmodel_em = sm.tsa.DynamicFactor(endog_em, k_factors=1, factor_order=1, error_order=1)\nresult_em = model_em.fit(disp=False)\n\nmodel_dm = sm.tsa.DynamicFactor(endog_dm, k_factors=1, factor_order=1, error_order=1)\nresult_dm = model_dm.fit(disp=False)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\ndf_est['factor_em'] = result_em.factors.smoothed[0]\ndf_est['factor_dm'] = result_dm.factors.smoothed[0]\n\nendog_global = df_est[['factor_em', 'factor_dm']]\n\nplt.plot(endog_global)\nplt.legend()\n\nmodel_global = sm.tsa.DynamicFactor(endog_global, k_factors=1, factor_order=1, error_order=1)\nresult_global = model_global.fit(disp=False)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  self._init_dates(dates, freq)\n\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\ndf_est['factor_global'] = result_global.factors.smoothed[0]\n\nplt.plot(df_est['factor_global'])\nplt.legend()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n<matplotlib.legend.Legend at 0x13ebeb6d0>\n\n\n\n\n\n\n# model = sm.tsa.DynamicFactorMQ(\n#     endog_m, endog_quarterly=endog_q,\n#     factors=factors, factor_orders=factor_orders,\n#     factor_multiplicities=factor_multiplicities)\nprint(df_est[['factor_em', 'factor_dm', 'factor_global']].tail(15))\nfrom scipy.stats import pearsonr\ncorr, _ = pearsonr(df_est['factor_em'], df_est['factor_dm'])\nprint('Pearsons correlation: %.3f' % corr)\n\ncurrency    factor_em  factor_dm  factor_global\ndate                                           \n2022-12-21  -0.059927  -0.081435       0.735832\n2022-12-22  -0.022280   0.005360       0.186527\n2022-12-23  -0.002162   0.017472      -0.014107\n2022-12-27   0.041217  -0.045417      -0.248542\n2022-12-28   0.022716  -0.107312       0.049828\n2022-12-29  -0.069029   0.119988       0.364497\n2022-12-30  -0.017344  -0.126943       0.454206\n2023-01-02  -0.000055   0.011553      -0.005445\n2023-01-03   0.156686   0.176762      -1.906100\n2023-01-04  -0.145457  -0.232314       1.955902\n2023-01-05   0.014978   0.019381      -0.183115\n2023-01-06   0.127845   0.171858      -1.619197\n2023-01-09  -0.202377  -0.372399       2.822352\n2023-01-10  -0.006585   0.084640      -0.153977\n2023-01-11  -0.029203  -0.029067       0.331555\nPearsons correlation: 0.698"
  },
  {
    "objectID": "posts/bayesian_regression/index.html",
    "href": "posts/bayesian_regression/index.html",
    "title": "Bayesian Regression - First Assessment",
    "section": "",
    "text": "This is my first post on Bayesian Econometrics.\n\nreal_estate = pd.read_csv('Real estate.csv')\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      No\n      X1 transaction date\n      X2 house age\n      X3 distance to the nearest MRT station\n      X4 number of convenience stores\n      X5 latitude\n      X6 longitude\n      Y house price of unit area\n    \n  \n  \n    \n      0\n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n    \n    \n      1\n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n    \n    \n      2\n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n    \n  \n\n\n\n\n\nreal_estate.columns = ['transaction_number', 'transaction_date', 'house_age', 'distance_mrt_station', 'convenience_stores', 'latitude', 'longitude', 'price_unit_area']\nreal_estate.set_index('transaction_number', inplace = True)\nreal_estate['intercept'] = 1\nreal_estate.head(3)\n\n\n\n\n\n  \n    \n      \n      transaction_date\n      house_age\n      distance_mrt_station\n      convenience_stores\n      latitude\n      longitude\n      price_unit_area\n      intercept\n    \n    \n      transaction_number\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2012.917\n      32.0\n      84.87882\n      10\n      24.98298\n      121.54024\n      37.9\n      1\n    \n    \n      2\n      2012.917\n      19.5\n      306.59470\n      9\n      24.98034\n      121.53951\n      42.2\n      1\n    \n    \n      3\n      2013.583\n      13.3\n      561.98450\n      5\n      24.98746\n      121.54391\n      47.3\n      1\n    \n  \n\n\n\n\n\nY = real_estate['price_unit_area'].to_numpy().reshape(-1,1)\nX_variables = ['intercept', 'house_age']\nX = real_estate[X_variables].to_numpy().reshape(-1, len(X_variables))\nX[1:5]\n\narray([[ 1. , 19.5],\n       [ 1. , 13.3],\n       [ 1. , 13.3],\n       [ 1. ,  5. ]])\n\n\n\nols_regression = LinearRegression(fit_intercept = False)\nols_regression.fit(X, Y)\nY_pred = ols_regression.predict(X)\n\nplt.scatter(X[:,1], Y)\nplt.plot(X[:,1], Y_pred, color='red')\nplt.xlabel('House Age')\nplt.ylabel('House Price of Unit Area')\nplt.show()\n\n\n\n\n\nalpha_ols, beta_ols = ols_regression.coef_[0]\nprint(alpha_ols, beta_ols)\n\n42.43469704626291 -0.25148841908534514"
  },
  {
    "objectID": "posts/fx_regression/index.html",
    "href": "posts/fx_regression/index.html",
    "title": "Currency Beta’s with respect to the USD",
    "section": "",
    "text": "This post is to check whether noise in daily data constitutes a problem to the estimation of currenies’ sensitivity with respect to the \\(USD\\), a common exercise done in financial markets.\nAssume that there exists a relationship of currency \\(x_{t}\\) with repect to the \\(USD\\) on a dily basis:\n\\[\n\\begin{aligned}\nx_{t} = \\alpha_{x} + \\beta_{x} USD_{t} + \\varepsilon_{t},\n\\end{aligned}\n\\]\nthis equation could be estimated to find the parameters of interest (a statistical relationship, with no causal meaning!).\nRemeber that the estimated coefficient is given by the simple formula:\n\\[\n\\begin{aligned}\n\\beta_{x} = \\frac{\\text{cov}(x_{t}, USD_{t})}{\\text{var}(USD_{t})}.\n\\end{aligned}\n\\]\nHowever, estimating this coefficint this way may face difficulties given the large variance of daily data. Given that, taken the parameters as fixed, nothing prevents one from resampling the data into a different frequency:\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_{t = 1}^{T}{x_{t}} & = \\frac{1}{T}\\sum_{t = 1}^{T}{\\alpha_{x}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\beta_{x} USD_{t}} + \\frac{1}{T}\\sum_{t = 1}^{T}{\\varepsilon_{t}} \\\\\n\\quad{} \\\\\n& \\Rightarrow x_{T} = \\alpha_{x} + \\beta_{x} USD_{T} + \\varepsilon_{T}\n\\end{aligned}\n\\]\nOur taks now is to test whether running the regressions over different sample frequencies yields distinct estimated coefficients.\n\n# Download and first inspection \n\ndf_daily = eurostat.get_data_df('ert_bil_eur_d', flags=False)\n\ndf_daily.tail(3)\n\n\n\n\n\n  \n    \n      \n      freq\n      statinfo\n      unit\n      currency\\TIME_PERIOD\n      1974-07-01\n      1974-07-02\n      1974-07-03\n      1974-07-04\n      1974-07-05\n      1974-07-08\n      ...\n      2023-01-09\n      2023-01-10\n      2023-01-11\n      2023-01-12\n      2023-01-13\n      2023-01-16\n      2023-01-17\n      2023-01-18\n      2023-01-19\n      2023-01-20\n    \n  \n  \n    \n      39\n      D\n      AVG\n      NAC\n      ZAR\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      18.2059\n      18.3054\n      18.2827\n      18.1417\n      18.2482\n      18.445\n      18.6027\n      18.396\n      18.6931\n      18.577\n    \n    \n      40\n      D\n      END\n      NAC\n      ALL\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      41\n      D\n      END\n      NAC\n      BAM\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3 rows × 12529 columns\n\n\n\n\n# Create a column with dates, select columns and filter values on interest\n\nlist_statinfo = ['AVG']\nlist_currency = ['AUD', 'BRL', 'CAD', 'CHF', 'CNY', 'CZK', 'GBP', 'HUF', 'IDR', 'JPY', 'MXN', 'NZD', 'PLN', 'TRY', 'USD', 'ZAR']\n\ndf_daily.rename(columns = {'currency\\\\TIME_PERIOD': 'currency'}, inplace = True)\n\ndf_daily = df_daily[(df_daily['statinfo'].isin(list_statinfo)) & \\\n     (df_daily['currency'].isin(list_currency))]\n\ndf_daily.drop(columns = ['unit', 'statinfo', 'freq'], inplace = True)\n\ndf_daily = pd.melt(df_daily, id_vars = ['currency'], var_name = 'date')\n\nprint(df_daily.tail(3))\nprint(df_daily.dtypes)\n\n       currency        date    value\n200397      TRY  2023-01-20  20.3566\n200398      USD  2023-01-20   1.0826\n200399      ZAR  2023-01-20  18.5770\ncurrency     object\ndate         object\nvalue       float64\ndtype: object\n\n\n\n# Transform date into the correct format and place currencies in columns\n\ndf_daily['date'] = df_daily['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\ndf_daily.set_index(['date', 'currency'], inplace = True)\n\ndf_daily = df_daily.unstack()\n\ndf_daily = df_daily.droplevel(level=0, axis=1)\n\ndf_daily.tail(3)\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      USD\n      ZAR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-18\n      1.5413\n      5.5252\n      1.4505\n      0.9906\n      7.3193\n      23.954\n      0.87530\n      396.15\n      16307.20\n      139.79\n      20.2193\n      1.6683\n      4.6983\n      20.3706\n      1.0839\n      18.3960\n    \n    \n      2023-01-19\n      1.5726\n      5.6326\n      1.4603\n      0.9921\n      7.3424\n      23.924\n      0.87648\n      396.43\n      16429.66\n      139.02\n      20.5437\n      1.6978\n      4.7063\n      20.3295\n      1.0815\n      18.6931\n    \n    \n      2023-01-20\n      1.5619\n      5.6271\n      1.4583\n      0.9962\n      7.3425\n      23.922\n      0.87600\n      395.88\n      16367.67\n      140.86\n      20.4865\n      1.6852\n      4.7100\n      20.3566\n      1.0826\n      18.5770\n    \n  \n\n\n\n\n\n# Put all currencies with respect to the USD (we downloaded currencies with respect to the EUR)\n\ndf_daily['EUR'] = 1\n\ndf_daily = df_daily.mul(1 / df_daily.loc[:,'USD'], axis = 0)\n\ndf_daily.drop('USD', axis = 1, inplace = True)\n\ndf_daily.tail(3)\n\n\n\n\n\n  \n    \n      currency\n      AUD\n      BRL\n      CAD\n      CHF\n      CNY\n      CZK\n      GBP\n      HUF\n      IDR\n      JPY\n      MXN\n      NZD\n      PLN\n      TRY\n      ZAR\n      EUR\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2023-01-18\n      1.421995\n      5.097518\n      1.338223\n      0.913922\n      6.752745\n      22.099825\n      0.807547\n      365.485746\n      15044.930344\n      128.969462\n      18.654212\n      1.539164\n      4.334625\n      18.793800\n      16.972045\n      0.922594\n    \n    \n      2023-01-19\n      1.454092\n      5.208137\n      1.350254\n      0.917337\n      6.789089\n      22.121128\n      0.810430\n      366.555710\n      15191.548775\n      128.543689\n      18.995562\n      1.569857\n      4.351641\n      18.797503\n      17.284420\n      0.924642\n    \n    \n      2023-01-20\n      1.442730\n      5.197765\n      1.347035\n      0.920192\n      6.782283\n      22.096804\n      0.809163\n      365.675226\n      15118.852762\n      130.112692\n      18.923425\n      1.556623\n      4.350637\n      18.803436\n      17.159616\n      0.923702\n    \n  \n\n\n\n\n\n# Dictionary with dataframes for analysis\n\nfreq_list = ['W', 'M', 'Q']\n\ndf_dict = {'D': df_daily.pct_change().dropna()}\n\ndef resample_data(df, freq):\n    \n    df_resampled = df.resample(freq) \\\n        .last() \\\n        .pct_change() \\\n        .dropna() \\\n        .to_period(freq)\n\n    return df_resampled\n\ndf_dict.update({freq: resample_data(df_daily, freq) for freq in freq_list})"
  }
]